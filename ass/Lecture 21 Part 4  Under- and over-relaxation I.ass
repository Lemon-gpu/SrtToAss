[Script Info]
; Script generated by Aegisub 3.2.2
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: None
PlayResX: 1080
PlayResY: 720

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,20,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: 辉英,Tahoma,35,&H00FFFFFF,&H000000FF,&H003B3C3D,&H00000000,0,0,0,0,70,70,1,0,1,1.3,0.2,2,0,0,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,00:00:00.000,00:00:13.840,辉英,,0,0,0,,okay so fortunately there is a almost a hack or modification to the Jacobi
Dialogue: 0,00:00:13.840,00:00:18.000,辉英,,0,0,0,,iteration and iterations like Jacobi iteration we can do to make it a good
Dialogue: 0,00:00:18.000,00:00:28.800,辉英,,0,0,0,,smoother this is called under relaxation what does it do so under relaxation is
Dialogue: 0,00:00:28.800,00:00:36.720,辉英,,0,0,0,,the following so we have performed remember we have said okay so u1 u of k
Dialogue: 0,00:00:37.440,00:00:48.000,辉英,,0,0,0,,is equal to u of k plus 1 is equal to Jacobi which is minus d inverse l plus u times
Dialogue: 0,00:00:54.560,00:00:58.560,辉英,,0,0,0,,so d inverse not this this is the
Dialogue: 0,00:00:58.800,00:01:02.960,辉英,,0,0,0,,solution error so minus d inverse times
Dialogue: 0,00:01:07.440,00:01:21.520,辉英,,0,0,0,,minus b plus l plus u times u of k right so that's the Jacobi iteration for for a u equal to b
Dialogue: 0,00:01:27.520,00:01:28.720,辉英,,0,0,0,,if we modify the
Dialogue: 0,00:01:28.800,00:01:38.800,辉英,,0,0,0,,iteration such that my u k plus 1 is not exactly equal to this but it is equal
Dialogue: 0,00:01:41.280,00:01:47.520,辉英,,0,0,0,,to an average between what i would get in a Jacobi iteration and the previous iteration
Dialogue: 0,00:01:48.720,00:01:56.560,辉英,,0,0,0,,so what i'm doing is i'm introducing a lambda here i call the relaxation factor
Dialogue: 0,00:01:58.160,00:01:58.720,辉英,,0,0,0,,i would be
Dialogue: 0,00:01:58.800,00:02:06.800,辉英,,0,0,0,,averaging using this relaxation factor 1 minus lambda times u k so that's the weight on the
Dialogue: 0,00:02:06.800,00:02:18.640,辉英,,0,0,0,,previous solution plus a lambda times what i would get if i'm applying the original Jacobi iteration
Dialogue: 0,00:02:19.920,00:02:24.960,辉英,,0,0,0,,times minus b plus l plus u times u k
Dialogue: 0,00:02:28.800,00:02:34.720,辉英,,0,0,0,,so what i have in the bracket is exactly what i would get if i apply the Jacobi iteration
Dialogue: 0,00:02:35.520,00:02:43.120,辉英,,0,0,0,,but i'm weighting that by lambda which if i use under relaxation lambda would be less than 1.
Dialogue: 0,00:02:43.120,00:02:48.960,辉英,,0,0,0,,so lambda less than 1 under relaxation
Dialogue: 0,00:02:51.760,00:02:57.920,辉英,,0,0,0,,there are other cases where you want lambda to be greater than 1. this is over
Dialogue: 0,00:02:57.920,00:03:06.000,辉英,,0,0,0,,over relaxation we are going to see later on applying over relaxation on Jacobi is a very bad
Dialogue: 0,00:03:06.000,00:03:12.720,辉英,,0,0,0,,idea but applying over relaxation on some other methods like Gauss-Seidel may be a good idea
Dialogue: 0,00:03:13.840,00:03:15.440,辉英,,0,0,0,,let's look at why that is the case
Dialogue: 0,00:03:18.480,00:03:26.720,辉英,,0,0,0,,okay so first of all we need to analyze in order for us to study the convergence how fast
Dialogue: 0,00:03:26.720,00:03:32.960,辉英,,0,0,0,,does the under relaxation or over relaxation converge for different frequency contents
Dialogue: 0,00:03:33.680,00:03:40.560,辉英,,0,0,0,,what should we do first how do we start analyzing convergence we need to derive
Dialogue: 0,00:03:41.360,00:03:48.880,辉英,,0,0,0,,a iteration for what quantity the error right we need to derive the error equation
Dialogue: 0,00:03:49.760,00:03:55.680,辉英,,0,0,0,,and what is the way to derive the error equation we subtract the iteration
Dialogue: 0,00:03:56.720,00:04:06.000,辉英,,0,0,0,,from a similar equation satisfied by the exact solution right in this case we know that the
Dialogue: 0,00:04:06.000,00:04:12.080,辉英,,0,0,0,,original iteration is satisfied by the exact solution and so is the original iteration
Dialogue: 0,00:04:12.080,00:04:21.360,辉英,,0,0,0,,multiplied by lambda so lambda u would be also equal to lambda times minus d inverse minus b
Dialogue: 0,00:04:21.840,00:04:26.320,辉英,,0,0,0,,plus l plus u times u right
Dialogue: 0,00:04:27.040,00:04:32.880,辉英,,0,0,0,,just multiplying the same equation that is the split of this split of this is d
Dialogue: 0,00:04:33.680,00:04:43.600,辉英,,0,0,0,,uh u plus l plus u times u is equal to b so if you move this to the right hand side and
Dialogue: 0,00:04:46.800,00:04:53.920,辉英,,0,0,0,,if you move this to the right hand side and multiply both sides by the inverse you're going
Dialogue: 0,00:04:53.920,00:04:58.900,辉英,,0,0,0,,going to get the the equation that is satisfied by u so this is multiplying
Dialogue: 0,00:04:58.900,00:05:13.100,辉英,,0,0,0,,lambda on both sides and then split this into two terms u plus u minus 1 minus
Dialogue: 0,00:05:13.100,00:05:18.880,辉英,,0,0,0,,lambda times u and then move this to the right hand side what we are going to get
Dialogue: 0,00:05:18.880,00:05:26.220,辉英,,0,0,0,,is u would be equal to 1 minus lambda times u plus lambda times the inverse
Dialogue: 0,00:05:26.220,00:05:37.140,辉英,,0,0,0,,minus B plus L plus u times u so the original the exact solution u
Dialogue: 0,00:05:37.140,00:05:43.680,辉英,,0,0,0,,satisfies a very similar equation to the iteration except for replacing all the
Dialogue: 0,00:05:43.680,00:05:47.880,辉英,,0,0,0,,K and K plus ones by the exact solution
Dialogue: 0,00:05:47.880,00:05:48.820,辉英,,0,0,0,,now
Dialogue: 0,00:05:48.820,00:05:58.060,辉英,,0,0,0,,subtract right so if we define UK to be UK minus you subtracting these equations
Dialogue: 0,00:05:58.060,00:06:07.420,辉英,,0,0,0,,would get me ek plus one is going to be equal to 1 minus lambda times ek plus
Dialogue: 0,00:06:07.420,00:06:13.940,辉英,,0,0,0,,lambda times the B term because they are the same in both is going to be canceled
Dialogue: 0,00:06:13.940,00:06:14.620,辉英,,0,0,0,,out
Dialogue: 0,00:06:14.620,00:06:18.800,辉英,,0,0,0,,I'm only left with minus the inverse times L plus
Dialogue: 0,00:06:18.800,00:06:31.040,辉英,,0,0,0,,you times you are times sorry you is going to be replaced by UK let's combine
Dialogue: 0,00:06:31.040,00:06:40.560,辉英,,0,0,0,,these two terms I would get one minus lambda times identity minus lambda times
Dialogue: 0,00:06:40.560,00:06:43.560,辉英,,0,0,0,,the inverse L plus you
Dialogue: 0,00:06:45.360,00:06:47.920,辉英,,0,0,0,,times ek
Dialogue: 0,00:06:47.920,00:06:57.160,辉英,,0,0,0,,therefore the under relaxation or over relaxation the effect of the relaxation
Dialogue: 0,00:06:57.160,00:07:07.100,辉英,,0,0,0,,to the error equation is to modifying this original iteration matrix by a
Dialogue: 0,00:07:07.100,00:07:15.680,辉英,,0,0,0,,constant multiplier lambda and then add that with a 1 minus lambda times identity
Dialogue: 0,00:07:15.680,00:07:16.680,辉英,,0,0,0,,identity
Dialogue: 0,00:07:18.800,00:07:23.560,辉英,,0,0,0,,Now think of what are these two things due to the eigenvalue of the matrix
Dialogue: 0,00:07:24.500,00:07:27.140,辉英,,0,0,0,,first of all what is a multiplication by lambda do
Dialogue: 0,00:07:27.140,00:07:30.600,辉英,,0,0,0,,what are the multiplication by lambda due to the eigenvalues of a matrix
Dialogue: 0,00:07:30.600,00:07:33.760,辉英,,0,0,0,,200
Dialogue: 0,00:07:33.760,00:07:35.640,辉英,,0,0,0,,hh
Dialogue: 0,00:07:35.640,00:07:42.220,辉英,,0,0,0,,multiplying by lambda right so a scaled version of the matrix has a scaled
Dialogue: 0,00:07:42.220,00:07:43.960,辉英,,0,0,0,,version of the eigenvalues
Dialogue: 0,00:07:43.960,00:07:44.820,辉英,,0,0,0,,1000
Dialogue: 0,00:07:44.820,00:07:46.560,辉英,,0,0,0,,Why Pat высокih chóney
Dialogue: 0,00:07:46.560,00:07:47.760,辉英,,0,0,0,,What is the different ?
Dialogue: 0,00:07:47.760,00:07:53.340,辉英,,0,0,0,,of adding an identity to the eigenvalue of the matrix.
Dialogue: 0,00:07:58.780,00:08:00.240,辉英,,0,0,0,,Make it more SPD.
Dialogue: 0,00:08:00.520,00:08:03.660,辉英,,0,0,0,,It's a constant shift to the eigenvalues, right?
Dialogue: 0,00:08:03.660,00:08:08.720,辉英,,0,0,0,,So let me call this iteration matrix by J, right?
Dialogue: 0,00:08:08.800,00:08:10.740,辉英,,0,0,0,,So let's call this J.
Dialogue: 0,00:08:10.920,00:08:14.340,辉英,,0,0,0,,I'm going to define this J by the original iteration matrix,
Dialogue: 0,00:08:14.440,00:08:15.660,辉英,,0,0,0,,which we know the eigenvalues.
Dialogue: 0,00:08:15.660,00:08:27.200,辉英,,0,0,0,,They are, so the eigenvalues of J is cosine of J pi over N.
Dialogue: 0,00:08:27.600,00:08:30.600,辉英,,0,0,0,,J goes from 1 to N minus 1, right?
Dialogue: 0,00:08:31.040,00:08:31.340,辉英,,0,0,0,,Okay.
Dialogue: 0,00:08:32.000,00:08:37.880,辉英,,0,0,0,,Now, the eigenvalue of the J lambda, I call it,
Dialogue: 0,00:08:37.880,00:08:43.480,辉英,,0,0,0,,the Jacobi iteration with under or over relaxation.
Dialogue: 0,00:08:45.240,00:08:45.640,辉英,,0,0,0,,Okay.
Dialogue: 0,00:08:45.660,00:08:55.660,辉英,,0,0,0,,So the eigenvalue of J is equal to 1 minus lambda I plus lambda times J, right?
Dialogue: 0,00:08:55.740,00:09:00.840,辉英,,0,0,0,,So we know, we basically, we have J multiplied by lambda and shifted by identity.
Dialogue: 0,00:09:01.320,00:09:10.920,辉英,,0,0,0,,So if I have J times eigenvector is equal to, I use the lambda here,
Dialogue: 0,00:09:11.080,00:09:14.240,辉英,,0,0,0,,so forgive me for using alpha as an eigenvalue.
Dialogue: 0,00:09:14.240,00:09:14.360,辉英,,0,0,0,,Okay.
Dialogue: 0,00:09:14.360,00:09:15.000,辉英,,0,0,0,,So if J is equal to 1 minus lambda I plus lambda times J,
Dialogue: 0,00:09:15.000,00:09:21.440,辉英,,0,0,0,,if Jv is equal to alpha v, then J lambda times v would be equal to,
Dialogue: 0,00:09:22.060,00:09:29.980,辉英,,0,0,0,,let's do the math, 1 minus lambda times identity times v plus this lambda times J times v.
Dialogue: 0,00:09:30.740,00:09:34.160,辉英,,0,0,0,,And this, because identity times v is just a v,
Dialogue: 0,00:09:34.300,00:09:39.940,辉英,,0,0,0,,I get 1 minus lambda times v plus lambda times alpha times v.
Dialogue: 0,00:09:40.400,00:09:42.880,辉英,,0,0,0,,So what I get is these are all constants.
Dialogue: 0,00:09:42.880,00:09:44.620,辉英,,0,0,0,,I get 1 minus lambda.
Dialogue: 0,00:09:45.000,00:09:48.720,辉英,,0,0,0,,So I get 1 minus lambda times v plus alpha lambda v.
Dialogue: 0,00:09:49.440,00:09:55.000,辉英,,0,0,0,,Therefore, v is still an eigenvector of the same, of J lambda,
Dialogue: 0,00:09:55.280,00:09:57.640,辉英,,0,0,0,,the under-relaxed iteration matrix.
Dialogue: 0,00:09:58.180,00:10:01.520,辉英,,0,0,0,,And the eigenvalues, originally being alpha,
Dialogue: 0,00:10:01.680,00:10:03.820,辉英,,0,0,0,,they are modified by multiplying with,
Dialogue: 0,00:10:04.360,00:10:09.940,辉英,,0,0,0,,multiplying with the under-relaxation factor and adding by 1 minus lambda.
Dialogue: 0,00:10:12.280,00:10:14.540,辉英,,0,0,0,,So these are the alphas we get.
Dialogue: 0,00:10:15.000,00:10:30.400,辉英,,0,0,0,,That means the eigenvalues of J lambda is going to be 1 minus lambda plus lambda times cosine J pi over n.
Dialogue: 0,00:10:31.780,00:10:32.360,辉英,,0,0,0,,Right?
Dialogue: 0,00:10:32.360,00:10:32.440,辉英,,0,0,0,,Right?
Dialogue: 0,00:10:38.120,00:10:44.240,辉英,,0,0,0,,So think of what this lambda does to these eigenvalues.
Dialogue: 0,00:10:45.000,00:10:52.960,辉英,,0,0,0,,First, let's imagine we have a previous eigenvalue that is very close to minus 1.
Dialogue: 0,00:10:52.960,00:10:54.760,辉英,,0,0,0,,That was our problem, right?
Dialogue: 0,00:10:54.760,00:10:59.480,辉英,,0,0,0,,That was our problem of why the J copy iteration is not suitable for multigrid.
Dialogue: 0,00:11:01.480,00:11:08.400,辉英,,0,0,0,,If this is very close to minus 1 and we apply a lambda that is close to 0.5.
Dialogue: 0,00:11:09.160,00:11:14.640,辉英,,0,0,0,,So let's imagine this is approximately minus 1 and this is approximately 0.5.
Dialogue: 0,00:11:15.000,00:11:15.680,辉英,,0,0,0,,What does this do?
Dialogue: 0,00:11:16.800,00:11:22.000,辉英,,0,0,0,,We have 1 minus 0.5, which is 0.5, plus 0.5 times minus 1.
Dialogue: 0,00:11:22.000,00:11:22.720,辉英,,0,0,0,,We get what?
Dialogue: 0,00:11:24.820,00:11:34.500,辉英,,0,0,0,,Or we are averaging something that is close to 0.1 with something that is 1, right?
Dialogue: 0,00:11:36.240,00:11:37.640,辉英,,0,0,0,,So this is a weighted average.
Dialogue: 0,00:11:37.640,00:11:43.000,辉英,,0,0,0,,You think of 1 minus lambda times 1 plus lambda times the original eigenvalue.
Dialogue: 0,00:11:44.200,00:11:44.980,辉英,,0,0,0,,This is a weighted average.
Dialogue: 0,00:11:45.000,00:11:48.400,辉英,,0,0,0,,This is an average between 1 and the original eigenvalues.
Dialogue: 0,00:11:49.440,00:11:52.360,辉英,,0,0,0,,What does it do to the negative eigenvalues?
Dialogue: 0,00:11:53.780,00:12:00.040,辉英,,0,0,0,,What does a lambda that is between 0 and 1 do to the eigenvalues?
Dialogue: 0,00:12:01.600,00:12:04.860,辉英,,0,0,0,,Yeah, it makes it, it pulls it towards 1.
Dialogue: 0,00:12:06.240,00:12:06.560,辉英,,0,0,0,,Right?
Dialogue: 0,00:12:08.820,00:12:10.900,辉英,,0,0,0,,Let me show the effect here.
Dialogue: 0,00:12:10.900,00:12:11.860,辉英,,0,0,0,,So lambda.
Dialogue: 0,00:12:13.200,00:12:14.320,辉英,,0,0,0,,Let's make.
Dialogue: 0,00:12:15.000,00:12:16.000,辉英,,0,0,0,,Okay, n.
Dialogue: 0,00:12:16.000,00:12:17.000,辉英,,0,0,0,,n equal to 16.
Dialogue: 0,00:12:17.000,00:12:22.240,辉英,,0,0,0,,Let's say we are going to see the same effect pretty soon.
Dialogue: 0,00:12:22.240,00:12:33.200,辉英,,0,0,0,,So let's say the eigenvalues are a, they are cosine of 1 to 15 times pi divided by n.
Dialogue: 0,00:12:33.200,00:12:34.200,辉英,,0,0,0,,Right?
Dialogue: 0,00:12:34.200,00:12:36.940,辉英,,0,0,0,,So these are my a's.
Dialogue: 0,00:12:36.940,00:12:42.120,辉英,,0,0,0,,And my lambda, so let's say is link space between 0 and 1.
Dialogue: 0,00:12:42.120,00:12:44.200,辉英,,0,0,0,,Actually, let's make it some over relaxation.
Dialogue: 0,00:12:44.200,00:12:45.200,辉英,,0,0,0,,0 to 2.
Dialogue: 0,00:12:45.200,00:12:46.200,辉英,,0,0,0,,Let's make it 100.
Dialogue: 0,00:12:46.200,00:12:53.200,辉英,,0,0,0,,Okay, so what I'm going to do is a, is going to show, I'm trying to plot lambda in the
Dialogue: 0,00:12:53.200,00:13:00.200,辉英,,0,0,0,,x-axis and the relaxed eigenvalues on the y-axis.
Dialogue: 0,00:13:00.200,00:13:01.200,辉英,,0,0,0,,Let me see.
Dialogue: 0,00:13:01.200,00:13:04.200,辉英,,0,0,0,,a is this, lambda is this.
Dialogue: 0,00:13:04.200,00:13:13.200,辉英,,0,0,0,,So what I want to do is my a lambda is going to be 1 minus lambda.
Dialogue: 0,00:13:13.200,00:13:14.200,辉英,,0,0,0,,Okay.
Dialogue: 0,00:13:14.200,00:13:22.200,辉英,,0,0,0,,So I'm going to plot lambda plus lambda times a, which I want to make it a transpose.
Dialogue: 0,00:13:22.200,00:13:23.200,辉英,,0,0,0,,Does this work?
Dialogue: 0,00:13:23.200,00:13:27.200,辉英,,0,0,0,,Lambda is going to be 1 by 100.
Dialogue: 0,00:13:27.200,00:13:28.200,辉英,,0,0,0,,So it's going to be this.
Dialogue: 0,00:13:28.200,00:13:30.200,辉英,,0,0,0,,a is going to be like that.
Dialogue: 0,00:13:30.200,00:13:31.200,辉英,,0,0,0,,No, this doesn't work.
Dialogue: 0,00:13:31.200,00:13:32.200,辉英,,0,0,0,,Okay.
Dialogue: 0,00:13:32.200,00:13:36.200,辉英,,0,0,0,,So I need to be lambda transpose times a.
Dialogue: 0,00:13:36.200,00:13:38.200,辉英,,0,0,0,,And this will be transpose.
Dialogue: 0,00:13:38.200,00:13:40.200,辉英,,0,0,0,,So let's see if that works.
Dialogue: 0,00:13:40.200,00:13:41.200,辉英,,0,0,0,,Okay.
Dialogue: 0,00:13:41.200,00:13:42.200,辉英,,0,0,0,,So a lambda is 100 by 15.
Dialogue: 0,00:13:42.200,00:13:43.200,辉英,,0,0,0,,So let's plot.
Dialogue: 0,00:13:43.200,00:13:47.200,辉英,,0,0,0,,Lambda a lambda.
Dialogue: 0,00:13:47.200,00:13:49.200,辉英,,0,0,0,,Yeah.
Dialogue: 0,00:13:49.200,00:13:50.200,辉英,,0,0,0,,Okay.
Dialogue: 0,00:13:50.200,00:13:51.200,辉英,,0,0,0,,Okay.
Dialogue: 0,00:13:51.200,00:13:52.200,辉英,,0,0,0,,So let's grade it.
Dialogue: 0,00:13:52.200,00:13:53.200,辉英,,0,0,0,,So here's what we get.
Dialogue: 0,00:13:53.200,00:14:08.200,辉英,,0,0,0,,When lambda is equal to 1, when lambda is equal to 1, that means that lambda is equal
Dialogue: 0,00:14:08.200,00:14:09.200,辉英,,0,0,0,,to 1.
Dialogue: 0,00:14:09.200,00:14:10.200,辉英,,0,0,0,,Okay.
Dialogue: 0,00:14:10.200,00:14:12.200,辉英,,0,0,0,,So that's what we get.
Dialogue: 0,00:14:12.200,00:14:18.200,辉英,,0,0,0,,That is the exact original Jacobi iteration, right?
Dialogue: 0,00:14:18.200,00:14:19.200,辉英,,0,0,0,,Lambda is equal to 1.
Dialogue: 0,00:14:19.200,00:14:20.200,辉英,,0,0,0,,This term is 0.
Dialogue: 0,00:14:20.200,00:14:21.200,辉英,,0,0,0,,Lambda is 1.
Dialogue: 0,00:14:21.200,00:14:24.200,辉英,,0,0,0,,So I get exactly the same Jacobi iteration.
Dialogue: 0,00:14:24.200,00:14:31.200,辉英,,0,0,0,,The eigenvalues goes from something very close to minus 1 to something very close to 1.
Dialogue: 0,00:14:31.200,00:14:35.200,辉英,,0,0,0,,This is responsible for the very slow convergence we observed.
Dialogue: 0,00:14:35.200,00:14:40.200,辉英,,0,0,0,,And this is responsible for very slow convergence of something that is difficult to observe
Dialogue: 0,00:14:40.200,00:14:41.200,辉英,,0,0,0,,generally.
Dialogue: 0,00:14:41.200,00:14:48.200,辉英,,0,0,0,,But like in multigrid, it's responsible for the slow convergence of multigrid.
Dialogue: 0,00:14:48.200,00:14:53.200,辉英,,0,0,0,,If we use a lambda that is greater than 1, what's going to happen?
Dialogue: 0,00:14:53.200,00:14:55.200,辉英,,0,0,0,,What do you think is going to happen to the iteration?
